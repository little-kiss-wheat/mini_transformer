"""
é¡¹ç›®ï¼šåŸºäºè½»é‡çº§Transformerçš„æ‰‹å†™æ–‡æœ¬åºåˆ—è¯†åˆ«
æ–‡ä»¶ï¼šmini_transformer.py
ä½œè€…ï¼šæ—æ³½è¿œ
æ—¥æœŸï¼š2026.02
åŠŸèƒ½ï¼šå®ç°é€‚é…MNISTçš„è½»é‡çº§Transformerï¼Œä»…ä¿ç•™æ ¸å¿ƒSelf-Attention/Encoder/ä½ç½®ç¼–ç 
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional

# ====================== é…ç½®ç±»ï¼ˆç»Ÿä¸€ç®¡ç†å‚æ•°ï¼Œç§‘ç ”é¡¹ç›®å¿…å¤‡ï¼‰ ======================
class MiniTransformerConfig:
    def __init__(
        self,
        img_size: int = 28,          # MNISTå›¾ç‰‡å°ºå¯¸
        in_channels: int = 1,        # è¾“å…¥é€šé“æ•°ï¼ˆç°åº¦å›¾=1ï¼‰
        num_classes: int = 10,       # åˆ†ç±»æ•°ï¼ˆMNIST=10ï¼‰
        embed_dim: int = 64,         # åµŒå…¥ç»´åº¦ï¼ˆè½»é‡åŒ–ï¼Œä¸ç”¨å¤§ï¼‰
        num_heads: int = 2,          # æ³¨æ„åŠ›å¤´æ•°ï¼ˆå¤§äºŒå…ˆè®¾2ï¼‰
        num_layers: int = 1,         # Encoderå±‚æ•°ï¼ˆè½»é‡åŒ–ï¼‰
        max_seq_len: int = 784       # åºåˆ—é•¿åº¦ï¼ˆ28*28=784ï¼‰
    ):
        self.img_size = img_size
        self.in_channels = in_channels
        self.num_classes = num_classes
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.max_seq_len = max_seq_len
        self.head_dim = embed_dim // num_heads  # æ¯ä¸ªå¤´çš„ç»´åº¦

# ====================== æ ¸å¿ƒæ¨¡å—1ï¼šè‡ªæ³¨æ„åŠ›ï¼ˆSelf-Attentionï¼‰ ======================
class SelfAttention(nn.Module):
    def __init__(self, config: MiniTransformerConfig):
        super().__init__()
        self.config = config

        # Q/K/V çº¿æ€§æŠ•å½±å±‚
        self.q_proj = nn.Linear(config.embed_dim, config.embed_dim)
        self.k_proj = nn.Linear(config.embed_dim, config.embed_dim)
        self.v_proj = nn.Linear(config.embed_dim, config.embed_dim)
        
        # è¾“å‡ºæŠ•å½±å±‚
        self.out_proj = nn.Linear(config.embed_dim, config.embed_dim)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        Args:
            x: è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ [batch_size, seq_len, embed_dim]
        Returns:
            æ³¨æ„åŠ›è¾“å‡ºï¼Œå½¢çŠ¶ [batch_size, seq_len, embed_dim]
        """
        batch_size, seq_len, _ = x.shape

        # 1. ç”ŸæˆQ/K/Vï¼Œå¹¶æ‹†åˆ†æ³¨æ„åŠ›å¤´
        q = self.q_proj(x).reshape(batch_size, seq_len, self.config.num_heads, self.config.head_dim).transpose(1, 2)
        k = self.k_proj(x).reshape(batch_size, seq_len, self.config.num_heads, self.config.head_dim).transpose(1, 2)
        v = self.v_proj(x).reshape(batch_size, seq_len, self.config.num_heads, self.config.head_dim).transpose(1, 2)

        # 2. ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›
        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.config.head_dim, dtype=torch.float32))
        attn_weights = F.softmax(attn_scores, dim=-1)  # æ³¨æ„åŠ›æƒé‡å½’ä¸€åŒ–

        # 3. åŠ æƒæ±‚å’Œ + æ‹¼æ¥æ³¨æ„åŠ›å¤´
        attn_output = torch.matmul(attn_weights, v).transpose(1, 2).reshape(batch_size, seq_len, self.config.embed_dim)
        
        # 4. è¾“å‡ºæŠ•å½±
        output = self.out_proj(attn_output)
        return output

# ====================== æ ¸å¿ƒæ¨¡å—2ï¼šTransformer Encoderå±‚ ======================
class TransformerEncoderLayer(nn.Module):
    def __init__(self, config: MiniTransformerConfig):
        super().__init__()
        self.config = config
        self.attn = SelfAttention(config)  # è‡ªæ³¨æ„åŠ›å±‚
        self.ffn = nn.Sequential(          # å‰é¦ˆç½‘ç»œï¼ˆè½»é‡åŒ–ç‰ˆï¼‰
            nn.Linear(config.embed_dim, config.embed_dim * 2),
            nn.ReLU(),
            nn.Linear(config.embed_dim * 2, config.embed_dim)
        )
        self.norm1 = nn.LayerNorm(config.embed_dim)  # å±‚å½’ä¸€åŒ–ï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
        self.norm2 = nn.LayerNorm(config.embed_dim)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­ï¼ˆå¸¦æ®‹å·®è¿æ¥ï¼‰"""
        # è‡ªæ³¨æ„åŠ› + æ®‹å·® + å½’ä¸€åŒ–
        x = self.norm1(x + self.attn(x))
        # å‰é¦ˆç½‘ç»œ + æ®‹å·® + å½’ä¸€åŒ–
        x = self.norm2(x + self.ffn(x))
        return x

# ====================== æ ¸å¿ƒæ¨¡å—3ï¼šä½ç½®ç¼–ç ï¼ˆTransformerå¿…å¤‡ï¼‰ ======================
class PositionalEncoding(nn.Module):
    def __init__(self, config: MiniTransformerConfig):
        super().__init__()
        self.config = config

        # ç”Ÿæˆä½ç½®ç¼–ç çŸ©é˜µï¼ˆå›ºå®šå€¼ï¼Œä¸å‚ä¸è®­ç»ƒï¼‰
        pe = torch.zeros(config.max_seq_len, config.embed_dim)
        position = torch.arange(0, config.max_seq_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, config.embed_dim, 2).float() * (-torch.log(torch.tensor(10000.0)) / config.embed_dim))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe)  # æ³¨å†Œä¸ºéè®­ç»ƒå‚æ•°

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """æ·»åŠ ä½ç½®ç¼–ç """
        x = x + self.pe[:x.size(1), :]
        return x

# ====================== æœ€ç»ˆï¼šMini Transformerï¼ˆé€‚é…MNISTï¼‰ ======================
class MiniTransformer(nn.Module):
    def __init__(self, config: Optional[MiniTransformerConfig] = None):
        super().__init__()
        # é»˜è®¤é…ç½®ï¼ˆæ²¡ä¼ å°±ç”¨è¿™ä¸ªï¼‰
        self.config = config if config is not None else MiniTransformerConfig()
        
        # 1. å›¾ç‰‡â†’åºåˆ—æŠ•å½±ï¼ˆ28*28*1 â†’ embed_dimï¼‰
        self.img2seq = nn.Linear(
            self.config.in_channels * self.config.img_size * self.config.img_size,
            self.config.embed_dim
        )
        
        # 2. ä½ç½®ç¼–ç 
        self.pos_enc = PositionalEncoding(self.config)
        
        # 3. Transformer Encoderï¼ˆå¤šå±‚å †å ï¼‰
        self.encoder = nn.Sequential(*[
            TransformerEncoderLayer(self.config) for _ in range(self.config.num_layers)
        ])
        
        # 4. åˆ†ç±»å¤´ï¼ˆåºåˆ—â†’åˆ†ç±»ç»“æœï¼‰
        self.classifier = nn.Linear(self.config.embed_dim, self.config.num_classes)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        
        batch_size = x.shape[0]
            
        # ğŸ”¥ ä¿®å¤ï¼šå…ˆå±•å¹³â†’æŠ•å½±ï¼ˆ784â†’64ï¼‰ï¼Œå†reshapeæˆåºåˆ—
        # 1. å±•å¹³å›¾ç‰‡ï¼š[batch_size, 784]
        x = x.flatten(1)
        # 2. å›¾ç‰‡æŠ•å½±åˆ°åµŒå…¥ç»´åº¦ï¼š[batch_size, 64]
        x = self.img2seq(x)
        # 3. reshapeæˆåºåˆ—ï¼š[batch_size, 1, 64]ï¼ˆseq_len=1ï¼Œå› ä¸ºæ˜¯å•å¼ å›¾ç‰‡åˆ†ç±»ï¼‰
        x = x.reshape(batch_size, 1, self.config.embed_dim)

        # 4. ä½ç½®ç¼–ç ï¼ˆseq_len=1ï¼Œç¼–ç ä¸å½±å“ï¼Œä½†ä¿ç•™é€»è¾‘ï¼‰
        x = self.pos_enc(x)

        # 5. Transformerç¼–ç 
        x = self.encoder(x)

        # 6. åºåˆ—å‡å€¼æ± åŒ– + åˆ†ç±»
        x = x.mean(dim=1)  # ç®€å•æ± åŒ–ï¼Œå¤§äºŒå¤Ÿç”¨
        output = self.classifier(x)

        return output

# ====================== æµ‹è¯•ä»£ç ï¼ˆéªŒè¯æ¨¡å‹å¯è¿è¡Œï¼‰ ======================
if __name__ == "__main__":
    # 1. åˆå§‹åŒ–é…ç½®
    config = MiniTransformerConfig(
        img_size=28,
        embed_dim=64,
        num_heads=2,
        num_layers=1
    )
    
    # 2. åˆå§‹åŒ–æ¨¡å‹
    model = MiniTransformer(config)
    
    # 3. æµ‹è¯•è¾“å…¥ï¼ˆMNISTæ ·ä¾‹ï¼‰
    dummy_input = torch.randn(4, 1, 28, 28)  # batch_size=4
    output = model(dummy_input)
    
    # 4. æ‰“å°å…³é”®ä¿¡æ¯ï¼ˆç§‘ç ”è°ƒè¯•å¿…å¤‡ï¼‰
    print(f"âœ… æ¨¡å‹åˆå§‹åŒ–æˆåŠŸï¼")
    print(f"è¾“å…¥å½¢çŠ¶: {dummy_input.shape}")
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape} (é¢„æœŸ: [4, 10])")
    print(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters())/1000:.2f}k")